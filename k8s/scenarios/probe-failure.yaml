# =============================================================================
# SCENARIO 6: Failed Liveness Probe - Health Check Failure
# =============================================================================
# This scenario deploys a service with a liveness probe that always fails,
# causing Kubernetes to continuously restart the pod.
#
# SRE Agent can diagnose: Pod being killed due to failed liveness probe,
# recommend checking application health endpoint or probe configuration.
#
# HOW TO BREAK:
# kubectl apply -f k8s/scenarios/probe-failure.yaml
#
# HOW TO FIX:
# kubectl delete deployment unhealthy-service -n pets
# =============================================================================
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: unhealthy-service
  namespace: pets
  labels:
    app: unhealthy-service
    scenario: probe-failure
    sre-demo: breakable
spec:
  replicas: 2
  selector:
    matchLabels:
      app: unhealthy-service
  template:
    metadata:
      labels:
        app: unhealthy-service
        scenario: probe-failure
    spec:
      nodeSelector:
        nodepool-type: user
      containers:
        - name: unhealthy-app
          image: mcr.microsoft.com/mirror/docker/library/nginx:alpine
          ports:
            - containerPort: 80
          resources:
            requests:
              memory: "32Mi"
              cpu: "25m"
            limits:
              memory: "64Mi"
              cpu: "50m"
          # BREAKING CHANGE: Liveness probe to endpoint that doesn't exist
          livenessProbe:
            httpGet:
              path: /health/nonexistent
              port: 80
            initialDelaySeconds: 5
            periodSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health/nonexistent
              port: 80
            initialDelaySeconds: 3
            periodSeconds: 3
